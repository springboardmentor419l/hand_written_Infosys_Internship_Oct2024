{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "EDGE DETECTION\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Original Image:\n",
      "[10, 10, 10, 10, 10]\n",
      "[10, 100, 100, 100, 10]\n",
      "[10, 100, 100, 100, 10]\n",
      "[10, 100, 100, 100, 10]\n",
      "[10, 10, 10, 10, 10]\n",
      "\n",
      "Vertical Edges Detected:\n",
      "[0, 0, 0, 0, 0]\n",
      "[0, 270, 0, 270, 0]\n",
      "[0, 360, 0, 360, 0]\n",
      "[0, 270, 0, 270, 0]\n",
      "[0, 0, 0, 0, 0]\n",
      "\n",
      "Horizontal Edges Detected:\n",
      "[0, 0, 0, 0, 0]\n",
      "[0, 270, 360, 270, 0]\n",
      "[0, 0, 0, 0, 0]\n",
      "[0, 270, 360, 270, 0]\n",
      "[0, 0, 0, 0, 0]\n"
     ]
    }
   ],
   "source": [
    "# Define the image array (2D list) representing a grayscale image\n",
    "image = [\n",
    "    [10, 10, 10, 10, 10],\n",
    "    [10, 100, 100, 100, 10],\n",
    "    [10, 100, 100, 100, 10],\n",
    "    [10, 100, 100, 100, 10],\n",
    "    [10, 10, 10, 10, 10]\n",
    "]\n",
    "\n",
    "# Define vertical and horizontal edge detection kernels (Sobel filters)\n",
    "vertical_kernel = [\n",
    "    [-1, 0, 1],\n",
    "    [-2, 0, 2],\n",
    "    [-1, 0, 1]\n",
    "]\n",
    "\n",
    "horizontal_kernel = [\n",
    "    [-1, -2, -1],\n",
    "    [0,  0,  0],\n",
    "    [1,  2,  1]\n",
    "]\n",
    "\n",
    "# Function to perform convolution\n",
    "def convolve(image, kernel):\n",
    "    rows = len(image)\n",
    "    cols = len(image[0])\n",
    "    k_size = len(kernel)\n",
    "    k_half = k_size // 2\n",
    "    \n",
    "    # Initialize the output array with zeros\n",
    "    output = [[0 for _ in range(cols)] for _ in range(rows)]\n",
    "    \n",
    "    # Perform convolution\n",
    "    for i in range(k_half, rows - k_half):\n",
    "        for j in range(k_half, cols - k_half):\n",
    "            weighted_sum = 0\n",
    "            for ki in range(k_size):\n",
    "                for kj in range(k_size):\n",
    "                    pixel = image[i + ki - k_half][j + kj - k_half]\n",
    "                    weight = kernel[ki][kj]\n",
    "                    weighted_sum += pixel * weight\n",
    "            output[i][j] = abs(weighted_sum)  # Taking absolute value for edge intensity\n",
    "    return output\n",
    "\n",
    "# Apply vertical and horizontal kernels\n",
    "vertical_edges = convolve(image, vertical_kernel)\n",
    "horizontal_edges = convolve(image, horizontal_kernel)\n",
    "\n",
    "# Display the resulting arrays\n",
    "print(\"Original Image:\")\n",
    "for row in image:\n",
    "    print(row)\n",
    "\n",
    "print(\"\\nVertical Edges Detected:\")\n",
    "for row in vertical_edges:\n",
    "    print(row)\n",
    "\n",
    "print(\"\\nHorizontal Edges Detected:\")\n",
    "for row in horizontal_edges:\n",
    "    print(row)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "LENET-5 BASICS"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "LeNet-5 Architecture\n",
    "Input Layer:\n",
    "\n",
    "Accepts grayscale images of size 32x32 pixels.\n",
    "MNIST images (28x28) are resized to fit.\n",
    "Convolutional Layer 1 (C1):\n",
    "\n",
    "6 filters of size 5x5 applied with a stride of 1.\n",
    "Output size: 28x28x6 (6 feature maps).\n",
    "Subsampling Layer 1 (S2):\n",
    "\n",
    "Average pooling with a 2x2 kernel and stride of 2.\n",
    "Output size: 14x14x6 (reduces spatial dimensions).\n",
    "Convolutional Layer 2 (C3):\n",
    "\n",
    "16 filters of size 5x5, connected selectively to previous layer maps.\n",
    "Output size: 10x10x16.\n",
    "Subsampling Layer 2 (S4):\n",
    "\n",
    "Another average pooling layer with a 2x2 kernel.\n",
    "Output size: 5x5x16.\n",
    "Fully Connected Layer 1 (F5):\n",
    "\n",
    "Input from previous layer flattened to 1D.\n",
    "120 neurons.\n",
    "Fully Connected Layer 2 (Output Layer):\n",
    "\n",
    "84 neurons in a dense layer, connected to 10 output neurons (one per digit class).\n",
    "Key Point\n",
    "Uses tanh activation instead of ReLU.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def build_lenet(input_shape):\n",
    "  # Define Sequential Model\n",
    "  model = tf.keras.Sequential()\n",
    "  \n",
    "  # C1 Convolution Layer\n",
    "  model.add(tf.keras.layers.Conv2D(filters=6, strides=(1,1), kernel_size=(5,5), activation='tanh', input_shape=input_shape))\n",
    "  \n",
    "  # S2 SubSampling Layer\n",
    "  model.add(tf.keras.layers.AveragePooling2D(pool_size=(2,2), strides=(2,2)))\n",
    "\n",
    "  # C3 Convolution Layer\n",
    "  model.add(tf.keras.layers.Conv2D(filters=6, strides=(1,1), kernel_size=(5,5), activation='tanh'))\n",
    "\n",
    "  # S4 SubSampling Layer\n",
    "  model.add(tf.keras.layers.AveragePooling2D(pool_size=(2,2), strides=(2,2)))\n",
    "\n",
    "  # C5 Fully Connected Layer\n",
    "  model.add(tf.keras.layers.Dense(units=120, activation='tanh'))\n",
    "\n",
    "  # Flatten the output so that we can connect it with the fully connected layers by converting it into a 1D Array\n",
    "  model.add(tf.keras.layers.Flatten())\n",
    "\n",
    "  # FC6 Fully Connected Layers\n",
    "  model.add(tf.keras.layers.Dense(units=84, activation='tanh'))\n",
    "\n",
    "  # Output Layer\n",
    "  model.add(tf.keras.layers.Dense(units=10, activation='softmax'))\n",
    "\n",
    "  # Compile the Model\n",
    "  model.compile(loss='categorical_crossentropy', optimizer=tf.keras.optimizers.SGD(lr=0.1, momentum=0.0, decay=0.0), metrics=['accuracy'])\n",
    "\n",
    "  return model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "CLASSIFICATION REPORT"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def calculate_metrics(actual_labels, predicted_labels, num_classes=10):\n",
    "    # Initialize counts\n",
    "    true_positive = [0] * num_classes\n",
    "    false_positive = [0] * num_classes\n",
    "    false_negative = [0] * num_classes\n",
    "\n",
    "    # Populate TP, FP, FN for each class\n",
    "    for i in range(len(actual_labels)):\n",
    "        true_label = int(actual_labels[i])\n",
    "        pred_label = int(predicted_labels[i])\n",
    "        if true_label == pred_label:\n",
    "            true_positive[true_label] += 1\n",
    "        else:\n",
    "            false_positive[pred_label] += 1\n",
    "            false_negative[true_label] += 1\n",
    "\n",
    "    # Calculate precision, recall, and F1-score for each class\n",
    "    precision = []\n",
    "    recall = []\n",
    "    f1_score = []\n",
    "\n",
    "    for i in range(num_classes):\n",
    "        tp = true_positive[i]\n",
    "        fp = false_positive[i]\n",
    "        fn = false_negative[i]\n",
    "\n",
    "        # Precision: TP / (TP + FP)\n",
    "        p = tp / (tp + fp) if (tp + fp) > 0 else 0.0\n",
    "        precision.append(p)\n",
    "\n",
    "        # Recall: TP / (TP + FN)\n",
    "        r = tp / (tp + fn) if (tp + fn) > 0 else 0.0\n",
    "        recall.append(r)\n",
    "\n",
    "        # F1-Score: 2 * (Precision * Recall) / (Precision + Recall)\n",
    "        f1 = 2 * (p * r) / (p + r) if (p + r) > 0 else 0.0\n",
    "        f1_score.append(f1)\n",
    "\n",
    "    # Calculate macro-average\n",
    "    macro_precision = sum(precision) / num_classes\n",
    "    macro_recall = sum(recall) / num_classes\n",
    "    macro_f1 = sum(f1_score) / num_classes\n",
    "\n",
    "    # Print results\n",
    "    print(\"Class\\tPrecision\\tRecall\\t\\tF1-Score\")\n",
    "    for i in range(num_classes):\n",
    "        print(f\"{i}\\t{precision[i]:.2f}\\t\\t{recall[i]:.2f}\\t\\t{f1_score[i]:.2f}\")\n",
    "\n",
    "    print(\"\\nMacro-Average Metrics:\")\n",
    "    print(f\"Precision: {macro_precision:.2f}\")\n",
    "    print(f\"Recall: {macro_recall:.2f}\")\n",
    "    print(f\"F1-Score: {macro_f1:.2f}\")\n",
    "\n",
    "    return precision, recall, f1_score, macro_precision, macro_recall, macro_f1\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "# Assuming act_labels and model_predicted are tensors from the test phase\n",
    "precision, recall, f1_score, macro_precision, macro_recall, macro_f1 = calculate_metrics(\n",
    "    act_labels.numpy(), model_predicted.numpy(), num_classes=10\n",
    ")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "CNN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Conv2D, MaxPooling2D, Flatten, Dense, Dropout\n",
    "from sklearn.metrics import precision_score, recall_score, f1_score\n",
    "import numpy as np\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def build_cnn():\n",
    "    model = Sequential([\n",
    "        # Convolutional layer\n",
    "        Conv2D(32, (3, 3), activation='relu', input_shape=(64, 64, 3)),\n",
    "        MaxPooling2D(pool_size=(2, 2)),\n",
    "        \n",
    "        # Another convolutional layer\n",
    "        Conv2D(64, (3, 3), activation='relu'),\n",
    "        MaxPooling2D(pool_size=(2, 2)),\n",
    "        \n",
    "        # Flattening\n",
    "        Flatten(),\n",
    "        \n",
    "        # Fully connected layers\n",
    "        Dense(128, activation='relu'),\n",
    "        Dropout(0.5),\n",
    "        Dense(2, activation='softmax')  # Output layer with 2 classes\n",
    "    ])\n",
    "    model.compile(optimizer='adam',\n",
    "                  loss='categorical_crossentropy',\n",
    "                  metrics=['accuracy'])\n",
    "    return model\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.keras.preprocessing.image import ImageDataGenerator\n",
    "\n",
    "train_datagen = ImageDataGenerator(rescale=1./255)\n",
    "test_datagen = ImageDataGenerator(rescale=1./255)\n",
    "\n",
    "train_generator = train_datagen.flow_from_directory(\n",
    "    'AutismDataset/train',\n",
    "    target_size=(64, 64),\n",
    "    batch_size=32,\n",
    "    class_mode='categorical'\n",
    ")\n",
    "\n",
    "test_generator = test_datagen.flow_from_directory(\n",
    "    'AutismDataset/test',\n",
    "    target_size=(64, 64),\n",
    "    batch_size=32,\n",
    "    class_mode='categorical',\n",
    "    shuffle=False\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
